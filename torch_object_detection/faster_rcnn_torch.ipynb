{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "* Data set creation and learning code adapted from the PyTorch tutorial [Torchvision Object Detection Finetuning Tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)\n",
    "* Code for plotting detection bounding boxes taken from the tutorial [Faster R-CNN Object Detection with PyTorch](https://www.learnopencv.com/faster-r-cnn-object-detection-with-pytorch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Dict\n",
    "\n",
    "import os\n",
    "import random\n",
    "import yaml\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ycb_label_data = {}\n",
    "with open('ycb_labels.yaml') as ycb_label_file:\n",
    "    ycb_label_data = yaml.load(ycb_label_file)\n",
    "category_names = ycb_label_data['category_names']\n",
    "label_map = ycb_label_data['label_map']\n",
    "num_classes = len(category_names)\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YCBDataset(object):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.data_list = []\n",
    "        for x in os.listdir(self.root):\n",
    "            dir_path = os.path.join(self.root, x)\n",
    "            if os.path.isdir(dir_path):\n",
    "                self.data_list.extend(self.__get_image_data(x, os.path.join(dir_path, 'images'),\n",
    "                                                            os.path.join(dir_path, 'masks')))\n",
    "\n",
    "    def __get_image_data(self, class_name: str, image_dir: str,\n",
    "                         mask_dir: str) -> Sequence[Dict[str, str]]:\n",
    "        '''Returns a list of dictionaries containing image-specific\n",
    "        data for all images from the specified class.  Each dictionary\n",
    "        in the resulting list is of the following format:\n",
    "        {\n",
    "            'img': path to an image (starting from self.root),\n",
    "            'mask': path to the image segmentation mask (starting from self.root),\n",
    "            'label': integer label of the class\n",
    "        }\n",
    "\n",
    "        Keyword arguments:\n",
    "        class_name: str -- name of the image class\n",
    "        image_dir: str -- name of a directory with RGB images\n",
    "        mask_dir: str -- name of a directory with image masks\n",
    "\n",
    "        '''\n",
    "        class_image_list = []\n",
    "        for x in os.listdir(image_dir):\n",
    "            name, _ = x.split('.')\n",
    "            image_data = {}\n",
    "            image_data['img'] = os.path.join(image_dir, name + '.jpg')\n",
    "            image_data['mask'] = os.path.join(mask_dir, name + '_mask.pbm')\n",
    "            image_data['label'] = label_map[class_name]\n",
    "            class_image_list.append(image_data)\n",
    "        return class_image_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = self.data_list[idx]['img']\n",
    "        mask_path = self.data_list[idx]['mask']\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img.thumbnail((600,600), Image.ANTIALIAS)\n",
    "\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "        mask.thumbnail((600,600), Image.ANTIALIAS)\n",
    "\n",
    "        # convert the PIL Image into a numpy array\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(1 - masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # we assume that all objects in the image are of the same class\n",
    "        labels = self.data_list[idx]['label'] * torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_prediction(img_path, threshold):\n",
    "    img = Image.open(img_path) # Load the image\n",
    "    transform = T.ToTensor() # Defing PyTorch Transform\n",
    "    img, _ = transform(img, None) # Apply the transform to the image\n",
    "    with torch.no_grad():\n",
    "        pred = model([img.to(device)]) # Pass the image to the model\n",
    "    pred_class = [category_names[i] for i in list(pred[0]['labels'].cpu().numpy())] # Get the Prediction Score\n",
    "    pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].cpu().detach().numpy())] # Bounding boxes\n",
    "    pred_score = list(pred[0]['scores'].cpu().detach().numpy())\n",
    "    pred_t = [pred_score.index(x) for x in pred_score if x > threshold][-1] # Get list of index with score greater than threshold.\n",
    "    pred_boxes = pred_boxes[:pred_t+1]\n",
    "    pred_class = pred_class[:pred_t+1]\n",
    "    return pred_boxes, pred_class\n",
    "\n",
    "def object_detection_api(img_path, threshold=0.5, rect_th=10, text_size=0.5, text_th=1):\n",
    "    boxes, pred_cls = get_prediction(img_path, threshold) # Get predictions\n",
    "    img = cv2.imread(img_path) # Read image with cv2\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Convert to RGB\n",
    "    for i in range(len(boxes)):\n",
    "        cv2.rectangle(img, boxes[i][0], boxes[i][1],color=(0, 255, 0), thickness=rect_th) # Draw Rectangle with the coordinates\n",
    "        cv2.putText(img,pred_cls[i], boxes[i][0],  cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th) # Write the prediction class\n",
    "    plt.figure(figsize=(20,30)) # display the output image\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "# dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
    "# dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
    "dataset = YCBDataset('ycb_dataset', get_transform(train=True))\n",
    "dataset_test = YCBDataset('ycb_dataset', get_transform(train=False))\n",
    "\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # evaluate on the test dataset\n",
    "    # evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "print('Training over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check - evaluating the detection on an image from the data set\n",
    "object_detection_api('/home/lucy/ycb_dataset/sponge/images/N1_252.jpg', threshold=0.4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
